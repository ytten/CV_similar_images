{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torch.nn import SyncBatchNorm\n",
    "from torchvision.models import resnet50, resnet152\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "target_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of left images: 2000\n",
      "\n",
      "Number of right images: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_image_paths(path):\n",
    "    image_names = []\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            fullpath = os.path.join(dirname, filename)\n",
    "            image_names.append(fullpath)\n",
    "    return image_names\n",
    "\n",
    "left_dir_path = \"ImgData/train/left\"\n",
    "right_dir_path = \"ImgData/train/right\"\n",
    "\n",
    "left_images_path = get_image_paths(left_dir_path)\n",
    "right_images_path = get_image_paths(right_dir_path)\n",
    "\n",
    "print(f\"Number of left images: {len(left_images_path)}\\n\")\n",
    "print(f\"Number of right images: {len(right_images_path)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images = left_images_path\n",
    "positive_images = right_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_tensor):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize(target_shape, antialias=True),  \n",
    "    ])\n",
    "    \n",
    "    image = transform(image_tensor)\n",
    "    return image\n",
    "\n",
    "def preprocess_triplets(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Given the filenames corresponding to the three images, load and\n",
    "    preprocess them.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(positive),\n",
    "        preprocess_image(negative),\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_doublets(anchor, test):\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(test),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_co_path(path, lookup, target_path):\n",
    "    file_name = os.path.basename(path)\n",
    "    src_img_name=os.path.splitext(file_name)[0]\n",
    "    co_img_name= lookup.loc[lookup['left']==src_img_name]['right']\n",
    "    co_img_name=co_img_name.values[0]\n",
    "    return os.path.join(target_path,co_img_name+'.jpg')\n",
    "    \n",
    "ilookup = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ImgData/train/right\\\\mqw.jpg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_co_path('ImgData/train/left/aaz.jpg', ilookup, right_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLLDataset(Dataset):\n",
    "    def __init__(self, anchor_image_paths, positive_image_paths, ilookup_path):\n",
    "        ilookup = pd.read_csv(ilookup_path)\n",
    "        anchor_images = [read_image(path) for path in anchor_image_paths]\n",
    "        positive_images = [read_image(find_co_path(path,ilookup,os.path.dirname(positive_image_paths[0]))) for path in anchor_image_paths]\n",
    "\n",
    "        negative_images = anchor_images + positive_images\n",
    "        random.shuffle(negative_images)\n",
    "        \n",
    "        self.a_images = torch.stack(anchor_images)\n",
    "        self.p_images = torch.stack(positive_images)\n",
    "        self.n_images = torch.stack(negative_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.a_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.a_images[idx]\n",
    "        positive = self.p_images[idx]\n",
    "        negative = self.n_images[idx]\n",
    "        \n",
    "        anchor, positive, negative = preprocess_triplets(anchor, positive, negative)\n",
    "        \n",
    "        return anchor, positive, negative\n",
    "\n",
    "ilookup_path = 'train.csv'\n",
    "tll_dataset = TLLDataset(anchor_images,positive_images,ilookup_path)\n",
    "\n",
    "image_num = len(tll_dataset)\n",
    "img_indices = list(range(image_num))\n",
    "train_indices = img_indices[:round(image_num * 0.8)]\n",
    "val_indices = img_indices[round(image_num * 0.8):]\n",
    "\n",
    "train_loader = DataLoader(tll_dataset, batch_size=32)\n",
    "val_loader = DataLoader(tll_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        resnet = resnet152(pretrained=True)\n",
    "        for name, param in resnet.named_parameters():\n",
    "            if \"layer4\" not in name:\n",
    "                param.requires_grad = False\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Sequential(nn.Linear(2048, 1024), nn.ReLU(), SyncBatchNorm(1024))\n",
    "        self.dense2 = nn.Sequential(nn.Linear(1024, 512), nn.ReLU(), SyncBatchNorm(512))\n",
    "        self.output = nn.Linear(512, 256)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "embedding_net = EmbeddingNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistanceLayer, self).__init__()\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        ap_distance = F.pairwise_distance(anchor, positive, 2)\n",
    "        an_distance = F.pairwise_distance(anchor, negative, 2)\n",
    "        return ap_distance, an_distance\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.distance_layer = DistanceLayer()\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor_embedding = self.embedding_net(anchor)\n",
    "        positive_embedding = self.embedding_net(positive)\n",
    "        negative_embedding = self.embedding_net(negative)\n",
    "        ap_distance, an_distance = self.distance_layer(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        return ap_distance, an_distance\n",
    "\n",
    "siamese_network = SiameseNetwork(embedding_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMarginLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, ap_distance, an_distance):\n",
    "        return F.relu(ap_distance - an_distance + self.margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "margin = 0.5\n",
    "\n",
    "siamese_network = siamese_network\n",
    "criterion = TripletMarginLoss(margin)\n",
    "optimizer = torch.optim.Adam(siamese_network.parameters(), lr=0.001)\n",
    "\n",
    "loss_tracker = []\n",
    "val_loss_tracker = []\n",
    "\n",
    "\n",
    "def train(train_loader, device):\n",
    "    epoch_loss = 0.0 \n",
    "    for anchor,positive,negative in train_loader:\n",
    "        anchor = anchor.to(device, dtype=torch.float)\n",
    "        positive = positive.to(device, dtype=torch.float)\n",
    "        negative = negative.to(device, dtype=torch.float)\n",
    "        \n",
    "        siamese_network.train() \n",
    "        optimizer.zero_grad()\n",
    "        ap_distance, an_distance = siamese_network(anchor, positive, negative)\n",
    "        loss = criterion(ap_distance, an_distance)\n",
    "\n",
    "        if loss.numel() > 1:\n",
    "            loss = loss.mean() \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_tracker.append(loss.item())\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# val \n",
    "def validate(val_loader,device):\n",
    "    epoch_loss = 0.0\n",
    "    for anchor,positive,negative in val_loader:\n",
    "        anchor = anchor.to(device, dtype=torch.float)\n",
    "        positive = positive.to(device, dtype=torch.float)\n",
    "        negative = negative.to(device, dtype=torch.float)\n",
    "        \n",
    "        siamese_network.eval()\n",
    "        with torch.no_grad():\n",
    "            ap_distance, an_distance = siamese_network(anchor, positive, negative)\n",
    "            loss = criterion(ap_distance, an_distance)\n",
    "\n",
    "            if loss.numel() > 1:\n",
    "                loss = loss.mean()  \n",
    "\n",
    "        val_loss_tracker.append(loss.item())\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    return epoch_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\ty\\CV_similar_images\\test_torch.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train_epoch(train_loader, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m val_epoch(val_loader, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\ty\\CV_similar_images\\test_torch.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m siamese_network\u001b[39m.\u001b[39mtrain() \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ap_distance, an_distance \u001b[39m=\u001b[39m siamese_network(anchor, positive, negative)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(ap_distance, an_distance)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m loss\u001b[39m.\u001b[39mnumel() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\ty\\CV_similar_images\\test_torch.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, anchor, positive, negative):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     anchor_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_net(anchor)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     positive_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_net(positive)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     negative_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_net(negative)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\ty\\CV_similar_images\\test_torch.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense1(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense2(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/test_torch.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(x)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:735\u001b[0m, in \u001b[0;36mSyncBatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_group:\n\u001b[0;32m    734\u001b[0m         process_group \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_group\n\u001b[1;32m--> 735\u001b[0m     world_size \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributed\u001b[39m.\u001b[39;49mget_world_size(process_group)\n\u001b[0;32m    736\u001b[0m     need_sync \u001b[39m=\u001b[39m world_size \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    738\u001b[0m \u001b[39m# fallback to framework BN when synchronization is not necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1067\u001b[0m, in \u001b[0;36mget_world_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[39mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[0;32m   1065\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1067\u001b[0m \u001b[39mreturn\u001b[39;00m _get_group_size(group)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:453\u001b[0m, in \u001b[0;36m_get_group_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39mHelper that gets a given group's world size.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m group \u001b[39mis\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD \u001b[39mor\u001b[39;00m group \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 453\u001b[0m     default_pg \u001b[39m=\u001b[39m _get_default_group()\n\u001b[0;32m    454\u001b[0m     \u001b[39mreturn\u001b[39;00m default_pg\u001b[39m.\u001b[39msize()\n\u001b[0;32m    455\u001b[0m \u001b[39mreturn\u001b[39;00m group\u001b[39m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:584\u001b[0m, in \u001b[0;36m_get_default_group\u001b[1;34m()\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[39mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[0;32m    582\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_initialized():\n\u001b[1;32m--> 584\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    585\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault process group has not been initialized, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mplease make sure to call init_process_group.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    587\u001b[0m     )\n\u001b[0;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "siamese_network = siamese_network.to(device)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(train_loader, device)\n",
    "    val_loss = validate(val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "    loss_tracker = []\n",
    "    val_loss_tracker = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(siamese_network, 'test_embed_152_pool.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classifier\n",
    "class PairwiseDistanceLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PairwiseDistanceLayer, self).__init__()\n",
    "\n",
    "    def forward(self, anchor, test):\n",
    "        distances = F.cosine_similarity(anchor, test,2)\n",
    "        return distances\n",
    "\n",
    "\n",
    "class outputlayer(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(outputlayer, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.distance_layer = PairwiseDistanceLayer()\n",
    "        \n",
    "    def forward(self, anchor, tests):        \n",
    "        anchor_embedding = self.embedding_net(anchor).unsqueeze(1).repeat(1,20,1)\n",
    "        tests_embedding = []\n",
    "\n",
    "        for i in range(tests.shape[1]):\n",
    "            current_test = tests[:, i, :, :, :]\n",
    "            tests_embedding.append(self.embedding_net(current_test).unsqueeze(1))\n",
    "        \n",
    "        tests_embedding = torch.cat(tests_embedding, dim=1)\n",
    "        distances = self.distance_layer(anchor_embedding, tests_embedding)\n",
    "        output = nn.Softmax(dim=1)(distances)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = pd.read_csv('test_candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jzn'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_table.drop('left', axis=1).to_numpy().flatten()[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self,test_anchor_dir, test_dir_paths, test_df_filename):\n",
    "        self.test_candidates_df = pd.read_csv(test_df_filename)\n",
    "        \n",
    "        anchor_names = self.test_candidates_df['left'].values\n",
    "\n",
    "        flatten_test_names = self.test_candidates_df.drop('left', axis=1).to_numpy().flatten()\n",
    "\n",
    "        anchor_paths = [os.path.join(test_anchor_dir, filename+'.jpg') for filename in anchor_names]\n",
    "        \n",
    "        test_anchor_images = [read_image(path) for path in anchor_paths]\n",
    "\n",
    "        # test_images_paths = [find_test_co_paths(path,test_dir_paths) for path in anchor_paths]\n",
    "\n",
    "        test_paths = [os.path.join(test_dir_paths, filename+'.jpg') for filename in flatten_test_names]\n",
    "        \n",
    "        test_images = [read_image(path) for path in test_paths]\n",
    "\n",
    "        self.test_anchor_images = torch.stack(test_anchor_images)\n",
    "        self.test_images = torch.stack(test_images)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_candidates_df.index)\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        anchor = self.test_anchor_images[idx]\n",
    "        test = self.test_images[idx*20: (idx+1)*20]\n",
    "\n",
    "        anchor, test = preprocess_doublets(anchor, test)\n",
    "\n",
    "        return anchor, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left_dir_path = \"ImgData/test/left\"\n",
    "test_right_dir_path = \"ImgData/test/right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testDataset(test_left_dir_path, test_right_dir_path,\"test_candidates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testset, batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNet = outputlayer(siamese_network.embedding_net).to('mps')\n",
    "\n",
    "result = pd.read_csv('test_candidates.csv')\n",
    "constant_batch_size = 32\n",
    "with torch.no_grad():\n",
    "    for i, (anchor, tests) in enumerate(test_loader):\n",
    "        \n",
    "        anchor = anchor.to('mps', dtype=torch.float)\n",
    "        tests = tests.to('mps', dtype=torch.float)\n",
    "    \n",
    "        sims = testNet(anchor, tests).tolist()\n",
    "        batch_size = len(sims)\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            result.loc[j+constant_batch_size*i, result.columns != 'left'] = sims[j]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"solutioncorrect152pretrainedpool.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
