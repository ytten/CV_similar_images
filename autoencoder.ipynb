{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler, random_split\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.nn import SyncBatchNorm\n",
    "from torchvision.models import resnet50, resnet152\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "target_shape = (200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of left images: 2000\n",
      "\n",
      "Number of right images: 2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getImagePaths(path):\n",
    "    image_names = []\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            fullpath = os.path.join(dirname, filename)\n",
    "            image_names.append(fullpath)\n",
    "    return image_names\n",
    "\n",
    "left_dir_path = \"ImgData/train/left\"\n",
    "right_dir_path = \"ImgData/train/right\"\n",
    "\n",
    "left_images_path = getImagePaths(left_dir_path)\n",
    "right_images_path = getImagePaths(right_dir_path)\n",
    "\n",
    "print(f\"Number of left images: {len(left_images_path)}\\n\")\n",
    "print(f\"Number of right images: {len(right_images_path)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShape(images_paths):\n",
    "    shape = cv2.imread(images_paths[0]).shape\n",
    "    for image_path in images_paths:\n",
    "        image_shape=cv2.imread(image_path).shape\n",
    "        if (image_shape!=shape):\n",
    "            return \"Different image shape\"\n",
    "        else:\n",
    "            return \"Same image shape \" + str(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_images = left_images_path\n",
    "positive_images = right_images_path\n",
    "\n",
    "train_imgs = anchor_images + positive_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Preprocess the input image tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the transformations: resize\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize(target_shape, antialias=True),  # Explicitly set antialias to True\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformations\n",
    "    # print(image_tensor)\n",
    "    image = transform(image_tensor)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_doublets(anchor, test):\n",
    "    return (\n",
    "        preprocess_image(anchor),\n",
    "        preprocess_image(test),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import sigmoid\n",
    "\n",
    "class TLLDataset(Dataset):\n",
    "    def __init__(self, train_imgs_paths):\n",
    "        # Load images and convert to tensors\n",
    "        train_imgs = [read_image(path) for path in train_imgs_paths]\n",
    "        # print(len(train_imgs_paths))\n",
    "        # positive_images = [read_image(find_co_path(path,ilookup,os.path.dirname(positive_image_paths[0]))) for path in anchor_image_paths]        \n",
    "        self.train_imgs = torch.stack(train_imgs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.train_imgs[idx]\n",
    "        img = preprocess_image(img)\n",
    "        # \n",
    "        img = img/255\n",
    "        # print(img)\n",
    "        return img\n",
    "\n",
    "# Create an instance of TLLDataset\n",
    "tll_dataset = TLLDataset(train_imgs)\n",
    "\n",
    "# Determine the indices for training and validation\n",
    "image_count = len(tll_dataset)\n",
    "indices = list(range(image_count))\n",
    "train_indices = indices[:round(image_count * 0.8)]\n",
    "val_indices = indices[round(image_count * 0.8):]\n",
    "\n",
    "# Create SubsetRandomSamplers\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(tll_dataset, batch_size=4, sampler=train_sampler)\n",
    "val_loader = DataLoader(tll_dataset, batch_size=4, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels = 3, hidden_channels =32, latent_dim=64, device='cuda'):\n",
    "        super(VAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(hidden_channels*2, latent_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        # Latent mean and variance\n",
    "        self.latent = nn.MaxPool2d()\n",
    "        \n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, hidden_channels*2, kernel_size=3, stride=2, padding=1, output_padding= 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(hidden_channels*2, hidden_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(hidden_channels, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # nn.ConstantPad2d((0, 1, 0, 1), value=0),  # Use ConstantPad2d for padding\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        # print(mean.shape)\n",
    "        return mean, logvar\n",
    "\n",
    "\n",
    "    def decode(self, x):\n",
    "        # print(x.shape)\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        x_hat = self.decode(z)  \n",
    "        # print(x_hat.shape)\n",
    "        # print('pred_image: ',x_hat[0][0])\n",
    "        return x_hat, mean, log_var\n",
    "    \n",
    "    def get_latent_space(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "\n",
    "        return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn.functional import normalize\n",
    "model = VAE().to('cuda')\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    \n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x)\n",
    "\n",
    "    # print('true label ', x[0][0])\n",
    "    # # if np.isnan(reproduction_loss.item()):\n",
    "    # #     print('true label ', x)\n",
    "    # #     print('pred_label ', x_hat)\n",
    "    # #     return\n",
    "\n",
    "    \n",
    "    # print('reproduction loss: ', reproduction_loss.item())\n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    # print('Mean: ', mean)\n",
    "    # print('log_var: ', log_var)\n",
    "    # print('KLD: ', KLD.item())\n",
    "    return 0.9*reproduction_loss + 0.1*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 1 \tAverage Loss:  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\ty\\CV_similar_images\\vae.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m, epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mAverage Loss: \u001b[39m\u001b[39m\"\u001b[39m, overall_loss\u001b[39m/\u001b[39m(batch_idx\u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m overall_loss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m train(model, optimizer, \u001b[39m5\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m, train_loader)\n",
      "\u001b[1;32mc:\\ty\\CV_similar_images\\vae.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m overall_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(device, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ty/CV_similar_images/vae.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x_hat, mean, log_var \u001b[39m=\u001b[39m model(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math \n",
    "def train(model, optimizer, epochs, device, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        for batch_idx, x in enumerate(train_loader):\n",
    "            \n",
    "            x = x.to(device, dtype=torch.float)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            overall_loss += loss.item()\n",
    "            # print(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*x.shape[0]))\n",
    "\n",
    "    return overall_loss\n",
    "\n",
    "train(model, optimizer, 5, 'cuda', train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model.decode(model.encode(next(iter(train_loader))))[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(model.decode(model.get_latent_space(batch.to('cuda'))).to('cpu').detach().numpy()[0][2], cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(batch[0][0], cmap='gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'vae.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classifier\n",
    "class PairwiseDistanceLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PairwiseDistanceLayer, self).__init__()\n",
    "\n",
    "    def forward(self, anchor, test):\n",
    "        # print(anchor)\n",
    "        # print(test)\n",
    "        \n",
    "        distances = F.cosine_similarity(anchor, test,2)\n",
    "        # print(distances)\n",
    "        return distances\n",
    "\n",
    "\n",
    "class outputlayer(nn.Module):\n",
    "    def __init__(self, vae_model):\n",
    "        super(outputlayer, self).__init__()\n",
    "        self.vae_model = vae_model\n",
    "        self.distance_layer = PairwiseDistanceLayer()\n",
    "        \n",
    "    def forward(self, anchor, tests):\n",
    "        anchor_embedding = self.vae_model.get_latent_space(anchor).unsqueeze(1).repeat(1,20,1)\n",
    "\n",
    "        tests_embedding = []\n",
    "        for i in range(tests.shape[1]):\n",
    "            current_test = tests[:, i, :, :]\n",
    "            tests_embedding.append(self.vae_model.get_latent_space(current_test).unsqueeze(1))\n",
    "        \n",
    "        tests_embedding = torch.cat(tests_embedding, dim=1)\n",
    "        print(tests_embedding[0][0])\n",
    "        distances = self.distance_layer(anchor_embedding, tests_embedding)\n",
    "        # print(distances)\n",
    "        output = nn.Softmax(dim=1)(distances)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testDataset(Dataset):\n",
    "    def __init__(self,test_anchor_dir, test_dir_paths, test_df_filename):\n",
    "        self.test_candidates_df = pd.read_csv(test_df_filename)\n",
    "        \n",
    "        anchor_names = self.test_candidates_df['left'].values\n",
    "\n",
    "        flatten_test_names = self.test_candidates_df.drop('left', axis=1).to_numpy().flatten()\n",
    "\n",
    "        anchor_paths = [os.path.join(test_anchor_dir, filename+'.jpg') for filename in anchor_names]\n",
    "        \n",
    "        test_anchor_images = [read_image(path) for path in anchor_paths]\n",
    "\n",
    "        # test_images_paths = [find_test_co_paths(path,test_dir_paths) for path in anchor_paths]\n",
    "\n",
    "        test_paths = [os.path.join(test_dir_paths, filename+'.jpg') for filename in flatten_test_names]\n",
    "        \n",
    "        test_images = [read_image(path) for path in test_paths]\n",
    "\n",
    "        self.test_anchor_images = torch.stack(test_anchor_images)\n",
    "        self.test_images = torch.stack(test_images)\n",
    "\n",
    "        # print(self.test_anchor_images.shape)\n",
    "        # print(self.test_images.shape)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_candidates_df.index)\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        anchor = self.test_anchor_images[idx]\n",
    "        test = self.test_images[idx*20: (idx+1)*20]\n",
    "\n",
    "        anchor, test = preprocess_doublets(anchor, test)\n",
    "\n",
    "        anchor = anchor/255\n",
    "        test = test/255\n",
    "\n",
    "        # print(self.test_candidates_df.iloc[idx])\n",
    "        return anchor, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_left_dir_path = \"ImgData/test/left\"\n",
    "test_right_dir_path = \"ImgData/test/right\"\n",
    "\n",
    "# test_left_images_path = getImagePaths(test_left_dir_path)\n",
    "# test_right_images_path = getImagePaths(test_right_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testDataset(test_left_dir_path, test_right_dir_path,\"test_candidates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testset, batch_size=4,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('test_candidates.csv')\n",
    "\n",
    "testNet = outputlayer(vae_model=model)\n",
    "\n",
    "constant_batch_size = 4\n",
    "with torch.no_grad():\n",
    "    for i, (anchor, tests) in enumerate(test_loader):\n",
    "        \n",
    "        anchor = anchor.to('cuda', dtype=torch.float)\n",
    "        tests = tests.to('cuda', dtype=torch.float)\n",
    "    \n",
    "        sims = testNet(anchor, tests).tolist()\n",
    "        # print(sims)\n",
    "        batch_size = len(sims)\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            result.loc[j+constant_batch_size*i, result.columns != 'left'] = sims[j]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"solutionVae.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
